{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_sentiment_analysis(tweet):\n",
    "    # removing punctuations\n",
    "    eng_punct = string.punctuation\n",
    "    def clean_punctuations(text):\n",
    "        trans = str.maketrans('', '', eng_punct)\n",
    "        return text.translate(trans)\n",
    "\n",
    "    # removing URLs\n",
    "    def cleaning_URLs(text):\n",
    "        return re.sub('((www.[^s]+)|(https?://[^s]+))', ' ', text)\n",
    "\n",
    "    # removing escape characters\n",
    "    def clean_escape_char(text):\n",
    "        return re.sub(\"\\r\\n\", \" \", text)\n",
    "\n",
    "    # Appliyng tokenization\n",
    "    # tokenizing involves splitting sentences and words from the body of the text.\n",
    "    def tokenization(text):\n",
    "        return (word_tokenize(text))\n",
    "\n",
    "    # Removing stopwords\n",
    "    # In natural language processing, useless words(data), are referred to as stop words.\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    def remove_stopwords(text):\n",
    "        text = [word for word in text if word not in stopword]\n",
    "        return text\n",
    "\n",
    "    # Applying Stemmer\n",
    "    # It provides the root of words. So you can eliminate words that come from the same root.\n",
    "    ps = nltk.PorterStemmer()\n",
    "    def stemming(text):\n",
    "        text = [ps.stem(word) for word in text]\n",
    "        return text\n",
    "\n",
    "    # Applying Lemmatizer\n",
    "    # the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.\n",
    "    lm = nltk.WordNetLemmatizer()\n",
    "    def lemmatizing(data):\n",
    "        text = [lm.lemmatize(word) for word in data]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    tweets_db = pd.DataFrame([[tweet]])\n",
    "    tweets_db.columns = [\"originalTweets\"]\n",
    "    tweets_db[\"cleanedTweets\"] = tweets_db.originalTweets.apply(\n",
    "        lambda x: clean_punctuations(x))\n",
    "    tweets_db[\"cleanedTweets\"] = tweets_db.cleanedTweets.apply(\n",
    "        lambda x: cleaning_URLs(x))\n",
    "    tweets_db[\"cleanedTweets\"] = tweets_db.cleanedTweets.apply(\n",
    "        lambda x: clean_escape_char(x))\n",
    "    tweets_db['cleanedTweets'] = tweets_db.cleanedTweets.apply(\n",
    "        lambda x: tokenization(x))\n",
    "    tweets_db['cleanedTweets'] = tweets_db.cleanedTweets.apply(\n",
    "        lambda x: remove_stopwords(x))\n",
    "    tweets_db['cleanedTweets'] = tweets_db.cleanedTweets.apply(\n",
    "        lambda x: stemming(x))\n",
    "    tweets_db['cleanedTweets'] = tweets_db.cleanedTweets.apply(\n",
    "        lambda x: lemmatizing(x))\n",
    "\n",
    "    # changing  the text to lowerCase\n",
    "    tweets_db[\"cleanedTweets\"] = tweets_db.cleanedTweets.str.lower()\n",
    "\n",
    "    analysis = TextBlob(tweets_db.cleanedTweets[0])\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "\n",
    "    # Give a sentiment intensity score to sentences.\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(\n",
    "        tweets_db.cleanedTweets[0])\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "\n",
    "    if neg > pos:\n",
    "        tweets_db['sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tweets_db['sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tweets_db['sentiment'] = \"neutral\"\n",
    "\n",
    "    tweets_db['polarity'] = polarity\n",
    "    tweets_db['subjectivity'] = subjectivity\n",
    "    tweets_db['neg'] = neg\n",
    "    tweets_db['neu'] = neu\n",
    "    tweets_db['pos'] = pos\n",
    "    tweets_db['compound'] = comp\n",
    "    tweets_db['text_len'] = tweets_db['cleanedTweets'].astype(str).apply(len)\n",
    "    tweets_db['text_word_count'] = tweets_db['cleanedTweets'].apply(\n",
    "        lambda x: len(str(x).split()))\n",
    "\n",
    "    return tweets_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(\"twitter_tweets_final_1.csv\")\n",
    "ADA_data = pd.read_csv(\"../preparedData/ADAUSDT.csv\")\n",
    "\n",
    "openTime = ADA_data[\"open_time\"]\n",
    "closeTime = ADA_data[\"close_time\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted openTime and closeTime from\n",
    "# dateFormat(\"%Y-%m-%d %H:%M:%S\") to timestamp\n",
    "openTime = [datetime.datetime.timestamp(\n",
    "                datetime.datetime.strptime(\n",
    "                    time, \"%Y-%m-%d %H:%M:%S\")\n",
    "                ) for time in openTime\n",
    "            ]\n",
    "\n",
    "closeTime = [datetime.datetime.timestamp(\n",
    "                datetime.datetime.strptime(\n",
    "                    time, \"%Y-%m-%d %H:%M:%S\")\n",
    "                ) for time in closeTime\n",
    "            ]\n",
    "\n",
    "# converted Timestamp column of twitter_data from\n",
    "# dateFormat(\"%Y-%m-%dT%H:%M:%S.%fZ\") to timestamp\n",
    "twitter_data.Timestamp = [datetime.datetime.timestamp(\n",
    "                datetime.datetime.strptime(\n",
    "                    time, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                ) for time in twitter_data.Timestamp\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating tweet_volume between the openTime and closeTime\n",
    "for i in range(len(openTime)):\n",
    "    tweets = twitter_data[\n",
    "        (twitter_data.Timestamp > openTime[i])\n",
    "        &\n",
    "        (twitter_data.Timestamp < closeTime[i])\n",
    "    ]\n",
    "    tweet_count = len(tweets)\n",
    "    \n",
    "    polarity, subjectivity, neg, neu, pos, comp, tweet_len, tweet_word_count = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    if(tweet_count > 0) :\n",
    "        # for twitter sentiment_analysis\n",
    "        for index, row in tweets[\"Text\"].iteritems():\n",
    "            sentiment_analysis = tweet_sentiment_analysis(row)\n",
    "            polarity += sentiment_analysis.polarity\n",
    "            subjectivity += sentiment_analysis.subjectivity\n",
    "\n",
    "            neg += sentiment_analysis.neg\n",
    "            neu += sentiment_analysis.neu\n",
    "            pos += sentiment_analysis.pos\n",
    "            comp += sentiment_analysis.compound\n",
    "\n",
    "            tweet_len += sentiment_analysis.text_len\n",
    "            tweet_word_count += sentiment_analysis.text_word_count\n",
    "        \n",
    "        # Adding new data in new columns of tweets in ADA_data\n",
    "        ADA_data.loc[i, 'tweet_count'] = tweet_count\n",
    "        ADA_data.loc[i, 'polarity'] = polarity[0]\n",
    "        ADA_data.loc[i, 'subjectivity'] = subjectivity[0]\n",
    "        ADA_data.loc[i, 'neg'] = neg[0]\n",
    "        ADA_data.loc[i, 'neu'] = neu[0]\n",
    "        ADA_data.loc[i, 'pos'] = pos[0]\n",
    "        ADA_data.loc[i, 'compound'] = comp[0]\n",
    "        ADA_data.loc[i, 'text_len'] = tweet_len[0]\n",
    "        ADA_data.loc[i, 'text_word_count'] = tweet_word_count[0]\n",
    "    else :\n",
    "        # Adding new data in new columns of tweets in ADA_data\n",
    "        ADA_data.loc[i, 'tweet_count'] = tweet_count\n",
    "        ADA_data.loc[i, 'polarity'] = polarity\n",
    "        ADA_data.loc[i, 'subjectivity'] = subjectivity\n",
    "        ADA_data.loc[i, 'neg'] = neg\n",
    "        ADA_data.loc[i, 'neu'] = neu\n",
    "        ADA_data.loc[i, 'pos'] = pos\n",
    "        ADA_data.loc[i, 'compound'] = comp\n",
    "        ADA_data.loc[i, 'text_len'] = tweet_len\n",
    "        ADA_data.loc[i, 'text_word_count'] = tweet_word_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save newly createddataFrame as .csv\n",
    "ADA_data.to_csv(\n",
    "    f\"../preparedData/ADAUSDT_twitter_sentiment.csv\", index=None, header=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e320152057eea021e47fe8cc1dfc9f3a6f51f00fb3f1a5070f08ecf7355f06e6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
